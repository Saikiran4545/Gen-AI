{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOjhR2pAULbCsg9OhIZ2822",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Saikiran4545/Gen-AI/blob/main/Basics_OF_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yUqJ2_G3VG28"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "NLP stands for the natural language processing. It is the branch of Ai which gives machine to understand and process human language"
      ],
      "metadata": {
        "id": "ycnMz6A3brsc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Components of NLP :-**\n",
        "\n",
        "\n",
        "There are two components\n",
        "\n",
        "1) Natural Language Understanding\n",
        "\n",
        "2)Natural language Generation"
      ],
      "metadata": {
        "id": "wH-PTjLtcmJL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Application Of NLP**\n",
        "\n",
        "The applications of Natural Language Processing are as follows:\n",
        "\n",
        "Text and speech processing like-Voice assistants – Alexa, Siri, etc.\n",
        "\n",
        "Text classification like Grammarly, Microsoft Word, and Google Docs\n",
        "\n",
        "Information extraction like-Search engines like DuckDuckGo, Google\n",
        "\n",
        "Chatbot and Question Answering like:- website bots\n",
        "\n",
        "Language Translation like:- Google Translate\n",
        "Text summarization"
      ],
      "metadata": {
        "id": "HF2WCtCWdNRV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NLP Libraries**\n",
        "\n",
        "1) NLTK\n",
        "\n",
        "2) Spacy\n",
        "\n",
        "3) Genism\n",
        "\n",
        "4)Fast Text"
      ],
      "metadata": {
        "id": "EOyuh6AJd2S5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Classical Approach for For NLP**\n",
        "\n",
        "\n",
        "1.   Regular Expression\n",
        "\n",
        "A regular expression (regex) is a sequence of characters that define a search pattern. Here’s how to write regular expressions:\n",
        "\n",
        "Start by understanding the special characters used in regex, such as “.”, “*”, “+”, “?”, and more.\n",
        "\n",
        "Choose a programming language or tool that supports regex, such as Python, Perl, or grep.\n",
        "\n",
        "Write your pattern using the special characters and literal characters.\n",
        "Use the appropriate function or method to search for the pattern in a string.\n",
        "\n",
        "\n",
        "2.  **Tokenization**\n",
        "\n",
        "Tokenization in natural language processing (NLP) is a technique that involves dividing a sentence or phrase into smaller units known as tokens. These tokens can encompass words, dates, punctuation marks, or even fragments of words.\n",
        "\n",
        "Tokenization is the process of dividing a text into smaller units known as tokens.\n",
        "\n",
        "Tokens are typically words or sub-words in the context of natural language processing.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FlOHtU2VDcsg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Types of Tokenization\n",
        "\n",
        "Tokenization can be classified into several types based on how the text is segmented. Here are some types of tokenization:\n",
        "\n",
        "Word Tokenization:\n",
        "Word tokenization divides the text into individual words. Many NLP tasks use this approach, in which words are treated as the basic units of meaning.\n",
        "\n",
        "Example:\n",
        "\n",
        "Input: \"Tokenization is an important NLP task.\"\n",
        "\n",
        "Output: [\"Tokenization\", \"is\", \"an\", \"important\", \"NLP\", \"task\", \".\"]\n",
        "\n",
        "\n",
        "Sentence Tokenization:\n",
        "The text is segmented into sentences during sentence tokenization. This is useful for tasks requiring individual sentence analysis or processing.\n",
        "\n",
        "Example:\n",
        "\n",
        "Input: \"Tokenization is an important NLP task. It helps break down text into smaller units.\"\n",
        "\n",
        "Output: [\"Tokenization is an important NLP task.\", \"It helps break down text into smaller units.\"]\n",
        "\n",
        "\n",
        "Subword Tokenization:\n",
        "\n",
        "Subword tokenization entails breaking down words into smaller units, which can be especially useful when dealing with morphologically rich languages or rare words.\n",
        "\n",
        "Example:\n",
        "\n",
        "Input: \"tokenization\"\n",
        "\n",
        "Output: [\"token\", \"ization\"]\n",
        "\n",
        "Character Tokenization:\n",
        "This process divides the text into individual characters. This can be useful for modelling character-level language.\n",
        "\n",
        "Example:\n",
        "\n",
        "Input: \"Tokenization\"\n",
        "\n",
        "Output: [\"T\", \"o\", \"k\", \"e\", \"n\", \"i\", \"z\", \"a\", \"t\", \"i\", \"o\", \"n\"]"
      ],
      "metadata": {
        "id": "mcn6CRzePyuQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "YjLi_yoWewAn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Need of Tokenization**\n",
        "Tokenization is a crucial step in text processing and natural language processing (NLP) for several reasons.\n",
        "\n",
        "Effective Text Processing: Tokenization reduces the size of raw text so that it can be handled more easily for processing and analysis.\n",
        "\n",
        "Feature extraction: Text data can be represented numerically for algorithmic comprehension by using tokens as features in machine learning models.\n",
        "\n",
        "Language Modelling: Tokenization in NLP facilitates the creation of organized representations of language, which is useful for tasks like text generation and language modelling.\n",
        "\n",
        "Information Retrieval: Tokenization is essential for indexing and searching in systems that store and retrieve information efficiently based on words or phrases.\n",
        "\n",
        "Text Analysis: Tokenization is used in many NLP tasks, including sentiment analysis and named entity recognition, to determine the function and context of individual words in a sentence.\n",
        "\n",
        "Vocabulary Management: By generating a list of distinct tokens that stand in for words in the dataset, tokenization helps manage a corpus’s vocabulary.\n",
        "Task-Specific Adaptation: Tokenization can be customized to meet the needs of particular NLP tasks, meaning that it will work best in applications such as summarization and machine translation.\n",
        "\n",
        "Preprocessing Step: This essential preprocessing step transforms unprocessed text into a format appropriate for additional statistical and computational analysis."
      ],
      "metadata": {
        "id": "mLdQlX1Dc_8k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Limitations of Tokenization**\n",
        "Tokenization is unable to capture the meaning of the sentence hence, results in ambiguity.\n",
        "\n",
        "In certain languages like Chinese, Japanese, Arabic, lack distinct spaces between words. Hence, there is an absence of clear boundaries that complicates the process of tokenization.\n",
        "\n",
        "Text may also include more than one word, for example email address, URLs and special symbols, hence it is difficult to decide how to tokenize such elements."
      ],
      "metadata": {
        "id": "Q8vAXD9kYJJ9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lemmatization**\n",
        "\n",
        "In contrast to stemming, lemmatization is a lot more powerful. It looks beyond word reduction and considers a language’s full vocabulary to apply a morphological analysis to words, aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma.\n",
        "\n",
        "Original Word ---> Root Word (lemma)      Feature\n",
        "\n",
        "   meeting    --->   meet                (core-word extraction)\n",
        "   was        --->    be                 (tense conversion to present tense)\n",
        "   mice       --->   mouse               (plural to singular)"
      ],
      "metadata": {
        "id": "OIiUrUm5XdyD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Various Approaches to Lemmatization:\n",
        "We will be going over 9 different approaches to perform Lemmatization along with multiple examples and code implementations.  \n",
        "\n",
        "WordNet\n",
        "\n",
        "WordNet (with POS tag)\n",
        "\n",
        "TextBlob\n",
        "\n",
        "TextBlob (with POS tag)\n",
        "\n",
        "spaCy\n",
        "\n",
        "TreeTagger\n",
        "\n",
        "Pattern\n",
        "\n",
        "Gensim\n",
        "\n",
        "Stanford CoreNLP"
      ],
      "metadata": {
        "id": "tRlApVtya3ir"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Wordnet Lemmatizer **\n",
        "Wordnet is a publicly available lexical database of over 200 languages that provides semantic relationships between its words. It is one of the earliest and most commonly used lemmatizer technique.  \n",
        "\n",
        "It is present in the nltk library in python.\n",
        "Wordnet links words into semantic relations.\n",
        "\n",
        "#> kites ---> kite\n",
        "#> babies ---> baby\n",
        "#> dogs ---> dog\n",
        "#> flying ---> flying\n",
        "#> smiling ---> smiling\n",
        "#> driving ---> driving\n",
        "#> died ---> died\n",
        "#> tried ---> tried\n",
        "#> feet ---> foot\n",
        "\n"
      ],
      "metadata": {
        "id": "giaJ3-AZbsua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Wordnet Lemmatizer (with POS tag)\n",
        "In the above approach, we observed that Wordnet results were not up to the mark. Words like ‘sitting’, ‘flying’ etc remained the same after lemmatization. This is because these words are treated as a noun in the given sentence rather than a verb. To overcome come this, we use POS (Part of Speech) tags.\n",
        "We add a tag with a particular word defining its type (verb, noun, adjective etc).\n",
        "\n",
        "\n",
        "For Example,\n",
        "\n",
        "#Word      +    Type (POS tag)     —>     Lemmatized Word\n",
        "#driving    +    verb      ‘v’            —>     drive\n",
        "#dogs       +    noun      ‘n’           —>     dog"
      ],
      "metadata": {
        "id": "-Li4enrfcQ8Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stemming**\n",
        "\n",
        "Stemming is a method in text processing that eliminates prefixes and suffixes from words, transforming them into their fundamental or root form, The main objective of stemming is to streamline and standardize words, enhancing the effectiveness of the natural language processing tasks. The article explores more on the stemming technique and how to perform stemming in Python.\n",
        "\n",
        "Simplifying words to their most basic form is called stemming, and it is made easier by stemmers or stemming algorithms.\n",
        "\n",
        " For example, “chocolates” becomes “chocolate” and “retrieval” becomes “retrieve.”\n",
        "\n",
        "  Some more example of stemming for root word \"like\" include:\n",
        "\n",
        "#\"likes\"\n",
        "#\"liked\"\n",
        "#\"likely\"\n",
        "#\"liking\"\n",
        "\n"
      ],
      "metadata": {
        "id": "ntxZ_jtvcalM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Types of Stemmer**\n",
        "\n",
        "1. Porter’s Stemmer\n",
        "\n",
        " It is one of the most popular stemming methods proposed in 1980. It is based on the idea that the suffixes in the English language are made up of a combination of smaller and simpler suffixes. This stemmer is known for its speed and simplicity.\n",
        "\n",
        " eg:-\n",
        "\n",
        "Original words: ['running', 'jumps', 'happily', 'running', 'happily']\n",
        "\n",
        "Stemmed words: ['run', 'jump', 'happili', 'run', 'happili']\n",
        "\n",
        "Advantage: It produces the best output as compared to other stemmers and it has less error rate.\n",
        "\n",
        "Limitation: Morphological variants produced are not always real words."
      ],
      "metadata": {
        "id": "ohSUQq-ei8wv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Lovins Stemmer\n",
        "\n",
        "It is proposed by Lovins in 1968, that removes the longest suffix from a word then the word is recorded to convert this stem into valid words.\n",
        "\n",
        "Example: sitting -> sitt -> sit\n",
        "\n",
        "Advantage: It is fast and handles irregular plurals like ‘teeth’ and ‘tooth’ etc.\n",
        "Limitation: It is time consuming and frequently fails to form words from stem."
      ],
      "metadata": {
        "id": "uLcyFlxUjV6h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Applications of Stemming**\n",
        "\n",
        "Stemming is used in information retrieval systems like search engines.\n",
        "\n",
        "It is used to determine domain vocabularies in domain analysis.\n",
        "\n",
        "To display search results by indexing while documents are evolving into numbers and to map documents to common subjects by stemming.\n",
        "\n",
        "Sentiment Analysis, which examines reviews and comments made by different users about anything, is frequently used for product analysis, such as for online retail stores. Before it is interpreted, stemming is accepted in the form of the text-preparation mean.\n",
        "\n",
        "A method of group analysis used on textual materials is called document clustering (also known as text clustering).\n",
        "\n",
        "Important uses of it include subject extraction, automatic document structuring, and quick information retrieval."
      ],
      "metadata": {
        "id": "D_BidXW9kO23"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stopwords**\n",
        "\n",
        "In natural language processing (NLP), stopwords are frequently filtered out to enhance text analysis and computational efficiency. Eliminating stopwords can improve the accuracy and relevance of NLP tasks by drawing attention to the more important words, or content words. The article aims to explore stopwords."
      ],
      "metadata": {
        "id": "6I-lFvc2mmR5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "print(stopwords.words('english'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-OPKU-g5n47g",
        "outputId": "33e81cb9-aa5d-4fad-db9b-b59aec614252"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Vectorization**\n",
        "\n",
        "Vectorization in NLP is the process of converting text data into numerical vectors that can be processed by machine learning algorithms.\n",
        "\n",
        "Vectorization is the process of converting text data into numerical vectors. In the context of Natural Language Processing (NLP), vectorization transforms words, phrases, or entire documents into a format that can be understood and processed by machine learning models. These numerical representations capture the semantic meaning and contextual relationships of the text, allowing algorithms to perform tasks such as classification, clustering, and prediction.\n",
        "\n",
        "\n",
        "#Machine Learning Compatibility:\n",
        "Machine learning models require numerical input to perform calculations. Vectorization converts text into a format that these models can process, enabling the application of statistical and machine learning techniques to textual data.\n",
        "#Capturing Semantic Meaning:\n",
        "Effective vectorization methods, like word embeddings, capture the semantic relationships between words. This allows models to understand context and perform better on tasks like sentiment analysis, translation, and summarization.\n",
        "\n",
        "#Dimensionality Reduction:\n",
        "Techniques like TF-IDF and word embeddings reduce the dimensionality of the data compared to one-hot encoding. This not only makes computation more efficient but also helps in capturing the most relevant features of the text.\n",
        "\n",
        "#Handling Large Vocabulary:\n",
        "Vectorization helps manage large vocabularies by creating fixed-size vectors for words or documents. This is essential for handling the vast amount of text data available in applications like search engines and social media analysis.\n",
        "\n",
        "#Improving Model Performance:\n",
        "Advanced vectorization techniques, such as contextualized embeddings, significantly enhance model performance by providing rich, context-aware representations of words. This leads to better generalization and accuracy in NLP tasks.\n",
        "\n",
        "#Facilitating Transfer Learning:\n",
        " Pre-trained models like BERT and GPT use vectorization to create embeddings that can be fine-tuned for various NLP tasks. This transfer learning approach saves time and resources by leveraging existing knowledge."
      ],
      "metadata": {
        "id": "_0fCQ9LVu1im"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Traditional Vectorization Techniques in NLP\n",
        "\n",
        "Here, we explore three traditional vectorization techniques: Bag of Words (BoW), Term Frequency-Inverse Document Frequency (TF-IDF), and Count Vectorizer.\n",
        "\n",
        "#1. Bag of Words (BoW)\n",
        "The Bag of Words model represents text by converting it into a collection of words (or tokens) and their frequencies, disregarding grammar, word order, and context. Each document is represented as a vector of word counts, with each element in the vector corresponding to the frequency of a specific word in the document.\n",
        "\n",
        "Advantages of Bag of Words (BoW)\n",
        "\n",
        "Simple and easy to implement.\n",
        "Provides a clear and interpretable representation of text.\n",
        "Disadvantages of Bag of Words (BoW)\n",
        "\n",
        "Ignores the order and context of words.\n",
        "Results in high-dimensional and sparse matrices.\n",
        "Fails to capture semantic meaning and relationships between words."
      ],
      "metadata": {
        "id": "8kO07CpOC5dt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. Term Frequency-Inverse Document Frequency (TF-IDF)\n",
        "\n",
        "TF-IDF is an extension of BoW that weighs the frequency of words by their importance across documents.\n",
        "\n",
        "Term Frequency (TF): Measures the frequency of a word in a document.\n",
        "\n",
        "\n",
        "TF(t, d) = \\frac{\\text{Number of times term } t \\text{ appears in document } d}{\\text{Total number of terms in document } d}\n",
        "\n",
        "Inverse Document Frequency (IDF):\n",
        "\n",
        "Measures the importance of a word across the entire corpus.\n",
        "\n",
        "IDF(t) = \\log \\left(\\frac{\\text{Total number of documents}}{\\text{Number of documents containing term } t}\\right)\n",
        "\n",
        "The TF-IDF score is the product of TF and IDF.\n",
        "\n",
        "\n",
        "Advantages of TF-IDF\n",
        "\n",
        "Reduces the impact of common words that appear frequently across documents.\n",
        "Helps in highlighting more informative and discriminative words.\n",
        "Disadvantages of TF-IDF\n",
        "\n",
        "Still results in sparse matrices.\n",
        "Does not capture word order or context.\n",
        "Computationally more expensive than BoW."
      ],
      "metadata": {
        "id": "spCjjpnlVASn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. Count Vectorizer\n",
        "\n",
        "The Count Vectorizer is similar to BoW but focuses on counting the occurrences of each word in the document. It converts a collection of text documents to a matrix of token counts, where each element represents the count of a word in a specific document.\n",
        "\n",
        "Advantages of Count Vectorizer\n",
        "\n",
        "Simple and straightforward implementation.\n",
        "Effective for tasks where word frequency is a key feature.\n",
        "Disadvantages of Count Vectorizer\n",
        "\n",
        "Similar to BoW, it produces high-dimensional and sparse matrices.\n",
        "Ignores the context and order of words.\n",
        "Limited ability to capture semantic meaning."
      ],
      "metadata": {
        "id": "Z5z1-_RBVb-h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "zwtaTgAFU87p"
      }
    }
  ]
}